<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pranita Sharma on Pranita Sharma</title>
    <link>https://pranita-s.github.io/</link>
    <description>Recent content in Pranita Sharma on Pranita Sharma</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Pursuit of Python</title>
      <link>https://pranita-s.github.io/post/searchpython/</link>
      <pubDate>Mon, 11 Jun 2018 00:00:00 +0530</pubDate>
      
      <guid>https://pranita-s.github.io/post/searchpython/</guid>
      <description>

&lt;p&gt;Pursuit of Reticulate package in Windows is tricky than searching its installation in Ubuntu/Mac. Windows operating system has disintegrated structure for file storage. It also has different drives such as C,D etc. which can be the home for software installation. Over this layer of complicated file structure comes the different ways in which Python can be installed in the Windows OS. It can be a manual installation of &lt;strong&gt;python.exe&lt;/strong&gt; or through frameworks such as &lt;strong&gt;Miniconda&lt;/strong&gt;, &lt;strong&gt;Anaconda&lt;/strong&gt;. This overlaying complications make the location of Python and its containing packages very unpredictable. Also, the search should be successful if the code in executed from the terminal or within IDE such as RStudio. I tried my best to go through proper research before writing my code for this convoluted &lt;strong&gt;pursuit&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;literature-survey&#34;&gt;Literature Survey&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The current code for checking Reticulate installation in Windows adds all predicatable paths in which Python could have been installed taking into consideration manual as well as platform based installations. Then, these are added to the RStudio&amp;rsquo;s environment PATH so that RStudio can locate &lt;strong&gt;Reticulate&lt;/strong&gt;. The underlying disadvantage with this approach is the fact that, we cannot predict all the paths where Python could have been installed. During manual installation of Python, it could have been possible that the user gave its custom created directory as its location. This path, cannot be predicted for adding in the list of possible paths. Therefore, a more robust and flexibe approach is needed for this intricate pursuit.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;PythonInR&lt;/strong&gt; is a R package which allows the user to interact with Python from R environment. I studied its source code to understand how it is locating Python as its crucial step for the successfull package working. It has a function named &lt;strong&gt;autodetectPython.R&lt;/strong&gt; which accomplishes this task. It tries to locate Python using &lt;strong&gt;where&lt;/strong&gt; command in the terminal then filter the searches according to the &amp;ldquo;arch&amp;rdquo; of R version. Henceforth, it tries of locate dll file of Python for final connection&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;approach-implemented&#34;&gt;Approach Implemented&lt;/h2&gt;

&lt;p&gt;The proposal I wrote for solving this problem used a command called &lt;strong&gt;which&lt;/strong&gt; from the terminal. I tried to emulate the same process of &lt;strong&gt;which&lt;/strong&gt; command of R. With the Windows 8, the terminal command was working like the command of R. The search needs to be carried out from the home directory containing the software to be searched. For example, if Reticulate is present in D drive, searching need to be started from the top most directory of the drive to find it. If the search begins from any other drive, say C, the search will be unsuccessful.&lt;/p&gt;

&lt;p&gt;But the trouble occurred, when I realised that, &amp;ldquo;which&amp;rdquo; terminal command was not present in Windows 10 and Windows 7 by default. Hence, I am currently using &lt;strong&gt;where&lt;/strong&gt; command to carry out the search for Reticulate. I have checked that it is present in all the Windows. The best part of &lt;strong&gt;where&lt;/strong&gt; command is that it can search for any entity irrespective of its current location. For example, if the current location is in C drive, an entity present in D Drive can be retrived. Therefore, it will take care of the different ways of Python installation as well as the complicated file structure of Windows.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Power of Reticulate</title>
      <link>https://pranita-s.github.io/post/reticulate/</link>
      <pubDate>Sun, 20 May 2018 00:00:00 +0530</pubDate>
      
      <guid>https://pranita-s.github.io/post/reticulate/</guid>
      <description>&lt;p&gt;It is been two weeks, since my last post about my GSoC selection. The first task assigned to me was writing a versatile install function for R API which will be at par in terms of functionalities of Python API. Earlier during my proposal preparation, I focussed on the need of configuration details required by databases such as &lt;strong&gt;PostgreSQL&lt;/strong&gt;,&lt;strong&gt;MySQL&lt;/strong&gt;,&lt;strong&gt;SQLite&lt;/strong&gt;. For instance, they could be made available through, &lt;strong&gt;configuration file (custom made by user or default file)&lt;/strong&gt; or in absence of this file, command the &lt;strong&gt;Retriever&lt;/strong&gt; to use defaults from code base.&lt;/p&gt;

&lt;p&gt;Writing code for the above workflow, which will take into account, different databases, is indeed an immaculate task. To combat such complexities, one of mentors, suggested &lt;a href=&#34;https://rstudio.github.io/reticulate/articles/introduction.html&#34; target=&#34;_blank&#34;&gt;Reticulate&lt;/a&gt; package. Its a package, that works wonders. Any package which resides in the world of Python and be imported in R and made to work in R environment.  After studying it theoretically, I was initially afraid of the execution speed, as it tries to combine two different worlds which dominate the space of Data Science. But, after giving it a number of trials, I fathomed that it executes pretty fast without any overhead, and can be incorporated directly to sync Python API and R API of &lt;strong&gt;Data Retriever&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I studied code of &lt;strong&gt;install function&lt;/strong&gt; of Python API deeply, to ensure that direct usage of its functions through &lt;strong&gt;Reticulate&lt;/strong&gt; encompasses all the detailed tasks which are required by R API. After variety of executions for different database supports, I figured that, using &lt;strong&gt;Reticulate&lt;/strong&gt; will be a boon. Synchronization of both the APIs requires minimal code and the pace of work has increased faster than ever. The Data Retriever might require an additional layer of detection of existance of Reticulate in the R environment as well.&lt;/p&gt;

&lt;p&gt;I expect to complete this task before coming weekend. In the coming week I plan to complete detection of &lt;strong&gt;Python&lt;/strong&gt; in &lt;strong&gt;Windows&lt;/strong&gt; machine for the Data Retriever.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Google Summer of Code 2018</title>
      <link>https://pranita-s.github.io/post/gsocselection/</link>
      <pubDate>Sat, 05 May 2018 00:00:00 +0530</pubDate>
      
      <guid>https://pranita-s.github.io/post/gsocselection/</guid>
      <description>&lt;p&gt;The clock struck 9:30 pm IST on 23rd April, 2018. I had been extremely impatient for this day since I submitted my proposal for &lt;a href=&#34;https://summerofcode.withgoogle.com/&#34; target=&#34;_blank&#34;&gt;Google Summer of Code&lt;/a&gt; on 27th March 2018. I checked my dashboard on the GSoC website and it showed my status as &lt;strong&gt;Active&lt;/strong&gt;. I was flummoxed after seeing it, as I did not really know what to comprehend -  Am I selected ? or Am I not ?. I started repeatedly refreshing my email account to witness an official email from Google stating my selection. After about 20 minutes, it was as it is, no new email. Well, it were really long 30 minutes, after which I received the email and then I had the tranquil realization, I had been waiting for, &lt;strong&gt;My Project Selection&lt;/strong&gt;. Subsequently, I received &amp;ldquo;congrats&amp;rdquo; from my mentors which made me even more happier.&lt;/p&gt;

&lt;p&gt;I applied for the &lt;a href=&#34;http://www.data-retriever.org/&#34; target=&#34;_blank&#34;&gt;RDataRetriever&lt;/a&gt; project under the &lt;a href=&#34;https://www.numfocus.org/&#34; target=&#34;_blank&#34;&gt;NumFocus&lt;/a&gt; umbrella. It is the R API for Data Retriever packages in Python. The Data Retriever handles downloading, pre-processing and storing of datasets in the user environment. It eases life of analysts and gives a head start by providing clean and normalized (analysis - ready) data with the goal that the users can reserve their time for studying/analyzing. I will be working on enhancing capabilities of R API and synchronizing it capabilities with the Python API.&lt;/p&gt;

&lt;p&gt;The awesome part of my project is the fact that its a &lt;strong&gt;CRAN package&lt;/strong&gt;. I have used R for almost two years during my tenure at R &amp;amp; D lab of Mphasis Inc and saying that CRAN packages are the most powerful weapon of R is an understatement. Therefore, I feel privileged to be contributing for the same, which will be accessible to other R and data analysis enthusiasts.&lt;/p&gt;

&lt;p&gt;The entire time period of prepping myself for GSoC to create a bespoke project proposal was a great learning exprience. I contributed to the RDataRetriever repo, had various conversations with the mentors,became enlightened of new tools, understood Open Source projects to a deeper level, and the list goes on. The exposure received from the GSoC selection process, invigorates me for a stimulating three months ahead ! I will be updating my progress through a post for elucidating my journey in GSoC 2018.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://pranita-s.github.io/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0530</pubDate>
      
      <guid>https://pranita-s.github.io/talk/example-talk/</guid>
      <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>InfraGraf</title>
      <link>https://pranita-s.github.io/project/infragraf/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pranita-s.github.io/project/infragraf/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I was part of this project during my tenure at Mphasis NEXT Labs as an Intern and Software Engineer.&lt;/li&gt;
&lt;li&gt;InfraGraf® is a Big Data complex event processing engine which enables enterprises to innovate and make strategic decisions regarding their technology infrastructure through actionable insights by correlation and causation analysis structured and unstructured data. It models enterprise technology infrastructure as complex systems consisting of interconnected servers, network devices, internet of things, industrial equipment etc. The powerful machine learning and graph theory based algorithms built into the platform identifies and predicts stand-alone as well as chain of events and incidents which could be related to system warnings, failures, outages, performance, availability and sub-optimal performances.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;My Role&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Leveraging power of Apache Spark Framework -

&lt;ul&gt;
&lt;li&gt;One of the data cleaning algorithms was taking more time than expected and an optimized approach for the same was crucial. I implemented the algorithm using Apache Spark in R programming language taking advantage of its distributed and parallel execution and robust implementation.&lt;/li&gt;
&lt;li&gt;For execution on cluster mode, I used AWS EC2 instances and S3 for storage. I also wrote python script for automating the process of cluster creation, code execution and cluster termination.&lt;/li&gt;
&lt;li&gt;It resulted in the decrease of the run time by 20 folds and was part of the product deployment.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Pattern Extraction from sequence data -

&lt;ul&gt;
&lt;li&gt;Assigned to a team of two.&lt;/li&gt;
&lt;li&gt;This is sub-project of the InfraGraf product. It is aimed at extracting all common patterns among given set event sequences with a minimum support and confidence. A common pattern is defined as a sequence which is subsequence among a defined minimum number of event sequences. Random walk based automata algorithm is built in C for this purpose. This algorithm is tested to give accurate results and implemented in real industrial applications.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Multi-server Architecture using Raspberry Pi</title>
      <link>https://pranita-s.github.io/project/raspberry/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pranita-s.github.io/project/raspberry/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This was my BE project. I was part of team of four.&lt;/li&gt;
&lt;li&gt;There has been an upsurge observed in the number of small scale and/or home-based businesses. This has been fueled by the internet which makes it very easy to set up one and offer services from the comfort of one’s home, accompanied by the reduction in costs of hardware devices in recent times. However, there are a myriad of networking needs of such offices in order to function smoothly, and most of them are expensive. Some of the most commonly needed ones are an always online file server, an intra-organization email server, a web server and a virtual private network service to stay secure online. We look at a way to cater to these networking needs by providing a solution which is cost effective and involves very low maintenance.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Our Solution&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To propose a fulfillment of the above requirements, we used Raspberry Pi. It is a mini computer which can be configured as Web Server, File Server, Email server and VPN at the same time. Being a cheaper alternative and having diverse capabailities, it posed as a great solution.&lt;/li&gt;
&lt;li&gt;We carried out its performace testing in the Central Computer Lab of our college with more than 150 students using the hosted servers at the same time.&lt;/li&gt;
&lt;li&gt;More details can be seen at &lt;a href=&#34;https://drive.google.com/file/d/1BiNaEVf0Hmayupoy5CkdALoDRIPvuvL8/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;https://drive.google.com/file/d/1BiNaEVf0Hmayupoy5CkdALoDRIPvuvL8/view?usp=sharing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Particle Swarm Optimization</title>
      <link>https://pranita-s.github.io/project/pso/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pranita-s.github.io/project/pso/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The goal was to exibit the power of PSO with respect to DE as a tuner for SVM. For this comparison, we (a team of three) referred the paper &amp;ldquo;Easy over hard - A Case Study on Deep Learning&amp;rdquo; for the problem statement, data and performance metrics. DE has crossover and mutation functions for its implementation. We chose PSO for comparison as it has simpler implementation which revolves around the two update equations for velocity and position of particle. We wrote the code for PSO from scratch in Python.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After testing PSO, we found that its performance metrics are at par with those of DE. But the time it takes for convergence is more, hence it is slightly slower than DE. It can be concluded that PSO can be used instead of DE, depending on the usecase and the important factors.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;My Role&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I contributed in the implementation of PSO algorithm in Python.&lt;/li&gt;
&lt;li&gt;Presentation for the same can be seen at - &lt;a href=&#34;https://docs.google.com/presentation/d/1FCd6igOw26W61A8BTVw7EHkDMXfhQ50d6YtcpuwGTMc/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;https://docs.google.com/presentation/d/1FCd6igOw26W61A8BTVw7EHkDMXfhQ50d6YtcpuwGTMc/edit?usp=sharing&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Search into Conversation</title>
      <link>https://pranita-s.github.io/project/alexa/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pranita-s.github.io/project/alexa/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Conversational user interfaces are becoming increasingly used for a variety of research needs.
Some are voice-activated (such as Alexa, Siri, and Google Now), but many are text-oriented chatbots appearing as assistants in the context of a larger application. Text chatbots are being considered for providing help in using our products, but also for improving the legal research experience
Given a set of case law and judge data, answer research questions using a text-oriented, conversational interface.
For instance, if a user asks &amp;ldquo;List cases for Judge Lucy Koh&amp;rdquo;, the system would respond with a list of cases where Judge Koh is listed as a presiding judge.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Test Cases&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;U: List cases handled by Judge ADAMS&lt;/li&gt;
&lt;li&gt;B: THERE ARE 6 JUDGES with Llast name ADAMS and 2 JUDGES with FIRST NAME&lt;/li&gt;
&lt;li&gt;U: Last name ADAMS&lt;/li&gt;
&lt;li&gt;B: There are only 2 JUDGES with last name ADAMS who are currently on service with first name Henry Lee in Florida and John R in state of OHIO.&lt;/li&gt;
&lt;li&gt;U: The one in state of OHIO&lt;/li&gt;
&lt;li&gt;B: There were 0 cases handled by the judge&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Team&amp;rsquo;s Solution&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We trained DialogFlow with the provided data and then integrated it with Amazon Alexa as a medium of conversation. To accomplish this we used DialogFlow&amp;rsquo;s Alexa Exporter and Amazon Developer Dashboard. After training DialogFlow with the data, we generated Alexa compatible files and then used these files to create a new skill for Alexa.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;My Role&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I trained DialogFlow with help of the data by creating appropriate intents, entities and actions.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Solving Kinematics Word Problem</title>
      <link>https://pranita-s.github.io/project/rnn/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pranita-s.github.io/project/rnn/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Problem Description&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Machine is fed with a Kinematics word problem. It has to parse and understand the problem, and decide which equation will be required to solve the problem from the three equations-

&lt;ul&gt;
&lt;li&gt;s = u + a*t&lt;/li&gt;
&lt;li&gt;v*v = u*u + 2*a*s&lt;/li&gt;
&lt;li&gt;s = u*t + 0.5*a*t*t&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;To achieve this, machine has to identify given entites such as velocity, displacement and time and also identify which entity has to be computed.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Existing related research papers concentrate on creating a question template to fit in the given entities and compute the missing one. But this prohibits the question solving capability of the machine to only free fall examples. Hence, I used RNN with LSTM network to train the machine with the questions and the label being the equation to solve it. I used NLP to make the understanding flexible as the machine has to identify details such as &amp;ldquo;at rest&amp;rdquo;,&amp;ldquo;initial velocity&amp;rdquo;,&amp;ldquo;final velocity&amp;rdquo;,&amp;ldquo;starting from rest&amp;rdquo; and also entites with different measuring  units such as metres per second, kilometers per hour etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recognition&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;I presented this work at Woman Who Code Conference held in Bangalore,India in March 2017 as a lightening talk speaker.&lt;/li&gt;
&lt;li&gt;More details about the work can be seen at &lt;a href=&#34;https://drive.google.com/file/d/1eWJSIztqPYhv__SGZO6onDPzCbP_i8Qb/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;https://drive.google.com/file/d/1eWJSIztqPYhv__SGZO6onDPzCbP_i8Qb/view?usp=sharing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Summarization of Document</title>
      <link>https://pranita-s.github.io/project/semantic/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pranita-s.github.io/project/semantic/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I was part of the team during my tenure at Mphasis NEXT Labs as a Software Engineer.&lt;/li&gt;
&lt;li&gt;This project is aimed to summarize grammatically written English document by build a queriable directed graph based on semantics and context (i.e. Event and Action). The query on graph can retrieve information about action and its effect, and entity and its role. We adopted various NLP and text mining methodologies to build the graph and stored the graph in Neo4j for effective information retrieval.&lt;/li&gt;
&lt;li&gt;This project involved usage of R and Python programming language, CoreNLP package for finding co-references among sentences, tokenization and POS tagging, Senna framework for Semantic Role Labelling, Semafor for frame-semantic parsing and Neo4j framework for graph querying.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;My Role&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I implemented co-reference relationship builder using CoreNLP (which internally uses Stanford CoreNLP framework) for replacement of pronouns with their respective nouns.&lt;/li&gt;
&lt;li&gt;I translated retrieved relationship between prominent nouns and verbs from the document pre-processing to Neo4j graph&lt;/li&gt;
&lt;li&gt;(Team of two) We created a GUI in RShiny for a prototype demonstration&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Person Re-Identification System For Mobile Devices</title>
      <link>https://pranita-s.github.io/publication/person-re-identification/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://pranita-s.github.io/publication/person-re-identification/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mobile visual clothing search</title>
      <link>https://pranita-s.github.io/publication/clothing-search/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://pranita-s.github.io/publication/clothing-search/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pranita-s.github.io/blog/getting_started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pranita-s.github.io/blog/getting_started/</guid>
      <description>&lt;p&gt;+++ title = &amp;ldquo;Academic: the website designer for Hugo&amp;rdquo;&lt;/p&gt;

&lt;p&gt;date = 2016-04-20T00:00:00 lastmod = 2018-01-13T00:00:00 draft = false&lt;/p&gt;

&lt;p&gt;tags = [&amp;ldquo;academic&amp;rdquo;] summary = &amp;ldquo;Create a beautifully simple website or blog in under 10 minutes.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;[header] image = &amp;ldquo;headers/getting-started.png&amp;rdquo; caption = &amp;ldquo;Image credit: Academic&amp;rdquo;&lt;/p&gt;

&lt;p&gt;[[gallery_item]] album = &amp;ldquo;1&amp;rdquo; image = &amp;ldquo;&lt;a href=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-default.png&amp;quot;&#34; target=&#34;_blank&#34;&gt;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-default.png&amp;quot;&lt;/a&gt; caption = &amp;ldquo;Default&amp;rdquo;&lt;/p&gt;

&lt;p&gt;[[gallery_item]] album = &amp;ldquo;1&amp;rdquo; image = &amp;ldquo;&lt;a href=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-ocean.png&amp;quot;&#34; target=&#34;_blank&#34;&gt;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-ocean.png&amp;quot;&lt;/a&gt; caption = &amp;ldquo;Ocean&amp;rdquo;&lt;/p&gt;

&lt;p&gt;[[gallery_item]] album = &amp;ldquo;1&amp;rdquo; image = &amp;ldquo;&lt;a href=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-dark.png&amp;quot;&#34; target=&#34;_blank&#34;&gt;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-dark.png&amp;quot;&lt;/a&gt; caption = &amp;ldquo;Dark&amp;rdquo;&lt;/p&gt;

&lt;p&gt;[[gallery_item]] album = &amp;ldquo;1&amp;rdquo; image = &amp;ldquo;&lt;a href=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-forest.png&amp;quot;&#34; target=&#34;_blank&#34;&gt;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-forest.png&amp;quot;&lt;/a&gt; caption = &amp;ldquo;Default&amp;rdquo;&lt;/p&gt;

&lt;p&gt;[[gallery_item]] album = &amp;ldquo;1&amp;rdquo; image = &amp;ldquo;&lt;a href=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-coffee-playfair.png&amp;quot;&#34; target=&#34;_blank&#34;&gt;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-coffee-playfair.png&amp;quot;&lt;/a&gt; caption = &amp;ldquo;Coffee theme with Playfair font&amp;rdquo;&lt;/p&gt;

&lt;p&gt;[[gallery_item]] album = &amp;ldquo;1&amp;rdquo; image = &amp;ldquo;&lt;a href=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-1950s.png&amp;quot;&#34; target=&#34;_blank&#34;&gt;https://raw.githubusercontent.com/gcushen/hugo-academic/master/images/theme-1950s.png&amp;quot;&lt;/a&gt; caption = &amp;ldquo;1950s&amp;rdquo; +++&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
